{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a2e6787aa53c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2 #this is not working but I need it to get working in the future \n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import L1L2\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the path and classes.\n",
    "directory = '../input/train'\n",
    "test_directory = '../input/test/'\n",
    "classes = ['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_path(*args,**kwargs):\n",
    "    import os\n",
    "    path_conv = \"\"\n",
    "    for i in range(len(args)):\n",
    "        \n",
    "        if len(args) == 1 :\n",
    "            path_conv = os.path.join(path_conv,args[i])     \n",
    "            return path_conv\n",
    "        elif i == (len(args) - 1):\n",
    "            return path_conv\n",
    "        \n",
    "        path_conv = os.path.join(path_conv,args[i],args[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the path and number of images to be displayed.\n",
    "# function plots those images.\n",
    "def display_images(path,no_of_images):\n",
    "    count = 1\n",
    "    for img in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "        plt.imshow(img_array, cmap='gray')\n",
    "        plt.show()\n",
    "        count += 1\n",
    "        if(no_of_images < count):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a shape to be used for our models.\n",
    "img_size1 = 240\n",
    "img_size2 = 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_and_test:\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        self.train_and_test = args\n",
    "        \n",
    "    # creating a training dataset.\n",
    "    def create_training_data(self,path,classes,img_size1,img_size2):\n",
    "            training_data = []\n",
    "            for img in tqdm(os.listdir(path)):\n",
    "                img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "                new_img = cv2.resize(img_array,(img_size2,img_size1))\n",
    "                if classes == 'c0':\n",
    "                    training_data.append([new_img,0])\n",
    "                elif classes == 'c1' :               \n",
    "                    training_data.append([new_img,1])\n",
    "                elif classes == 'c2' :               \n",
    "                    training_data.append([new_img,1])\n",
    "                elif classes == 'c3' :               \n",
    "                    training_data.append([new_img,1])\n",
    "                elif classes == 'c4' :               \n",
    "                    training_data.append([new_img,1])\n",
    "                elif classes == 'c5' :               \n",
    "                    training_data.append([new_img,1])\n",
    "                elif classes == 'c6' :               \n",
    "                    training_data.append([new_img,1])\n",
    "                elif classes == 'c7' :               \n",
    "                    training_data.append([new_img,1])\n",
    "                elif classes == 'c8' :               \n",
    "                    training_data.append([new_img,1])\n",
    "                elif classes == 'c9' :               \n",
    "                    training_data.append([new_img,1])\n",
    "            return training_data\n",
    "        \n",
    "    # Creating a test dataset.    \n",
    "    def create_testing_data(self,path,img_size1,img_size2):\n",
    "        testing_data = []       \n",
    "        for img in tqdm(os.listdir(path)):\n",
    "            img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "            new_img = cv2.resize(img_array,(img_size2,img_size1))\n",
    "            testing_data.append([img,new_img])\n",
    "        return testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the train and test classes for training and validation.\n",
    "_train_ = train_and_test()\n",
    "_test_ = train_and_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_c0 = _train_.create_training_data(generate_path(directory,classes[0]),classes[0],img_size1,img_size2)\n",
    "training_data_c1 = _train_.create_training_data(generate_path(directory,classes[1]),classes[1],img_size1,img_size2)\n",
    "training_data_c2 = _train_.create_training_data(generate_path(directory,classes[2]),classes[2],img_size1,img_size2)\n",
    "training_data_c3 = _train_.create_training_data(generate_path(directory,classes[3]),classes[3],img_size1,img_size2)\n",
    "training_data_c4 = _train_.create_training_data(generate_path(directory,classes[4]),classes[4],img_size1,img_size2)\n",
    "training_data_c5 = _train_.create_training_data(generate_path(directory,classes[5]),classes[5],img_size1,img_size2)\n",
    "training_data_c6 = _train_.create_training_data(generate_path(directory,classes[6]),classes[6],img_size1,img_size2)\n",
    "training_data_c7 = _train_.create_training_data(generate_path(directory,classes[7]),classes[7],img_size1,img_size2)\n",
    "training_data_c8 = _train_.create_training_data(generate_path(directory,classes[8]),classes[8],img_size1,img_size2)\n",
    "training_data_c9 = _train_.create_training_data(generate_path(directory,classes[9]),classes[9],img_size1,img_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = _test_.create_testing_data(generate_path(test_directory),img_size1,img_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train and test data for our model \n",
    "class features_and_labels:\n",
    "    # get all the arguments dynmically.\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        self.features_and_labels = args\n",
    "        \n",
    "    # generate your features and labels.\n",
    "    def generate_features_and_label(self,_class1_,_class2_):\n",
    "        x = []\n",
    "        y = []\n",
    "        \n",
    "        for features, label in tqdm(_class1_):\n",
    "            x.append(features)\n",
    "            y.append(label)\n",
    "            \n",
    "        for features, label in tqdm(_class2_):\n",
    "            x.append(features)\n",
    "            y.append(label)\n",
    "            \n",
    "        return x,y\n",
    "    \n",
    "    # generate np_arrays for test.\n",
    "    def generate_npArray(self,_class1_,_class2_,img_size2,img_size1) :\n",
    "        x,y = self.generate_features_and_label(_class1_,_class2_)\n",
    "        np_array = np.array(x).reshape(-1,img_size2*img_size1)\n",
    "        return np_array, y\n",
    "    \n",
    "    # train and split your data.\n",
    "    def train_and_split(self,features,labels,test_size,random_state,num_class):\n",
    "        x_train,x_test,y_train,y_test = train_test_split(features,labels,test_size=test_size,random_state=random_state)\n",
    "        Y_train = np_utils.to_categorical(y_train,num_classes=num_class)\n",
    "        Y_test = np_utils.to_categorical(y_test,num_classes=num_class)\n",
    "        \n",
    "        return x_train,x_test,Y_train,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data_c0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-37661c5a4850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeature_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnp_array_c0c1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_c0c1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_npArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_c0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_data_c1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_size2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_size1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mx_train_c0c1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test_c0c1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_c0c1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_c0c1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_array_c0c1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_c0c1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_data_c0' is not defined"
     ]
    }
   ],
   "source": [
    "#creating training data for safe vs texting_right\n",
    "\n",
    "feature_label = features_and_labels()\n",
    "\n",
    "np_array_c0c1,y_c0c1 = feature_label.generate_npArray(training_data_c0,training_data_c1,img_size2,img_size1)\n",
    "x_train_c0c1,x_test_c0c1,y_train_c0c1,y_test_c0c1 = feature_label.train_and_split(np_array_c0c1,y_c0c1,0.3,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the logistic regression classifier.\n",
    "output_dim = nb_classes = 2\n",
    "batch_size = 128 \n",
    "nb_epoch = 100\n",
    "model_c0c1 = Sequential() \n",
    "model_c0c1.add(BatchNormalization())\n",
    "model_c0c1.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \n",
    "model_c0c1.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "callbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\n",
    "history_c0c1 = model_c0c1.fit(x_train_c0c1, y_train_c0c1, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c1, y_test_c0c1),callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c0c1.save_weights('./driverdistraction_Safe_vs_texting_right_weights.h5', overwrite=True)\n",
    "model_c0c1.save('./driverdistraction_lr_Safe_vs_texting_right.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history_c0c1.history['acc'])\n",
    "plt.plot(history_c0c1.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history_c0c1.history['loss'])\n",
    "plt.plot(history_c0c1.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating training data for safe vs talking_on_the_phone_right\n",
    "np_array_c0c2, y_c0c2 = feature_label.generate_npArray(training_data_c0,training_data_c2,img_size2,img_size1)\n",
    "x_train_c0c2,x_test_c0c2,y_train_c0c2,y_test_c0c2 = feature_label.train_and_split(np_array_c0c2,y_c0c2,0.3,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the logistic regression classifier.\n",
    "output_dim = nb_classes = 2\n",
    "batch_size = 128 \n",
    "nb_epoch = 25\n",
    "model_c0c2 = Sequential() \n",
    "model_c0c2.add(BatchNormalization())\n",
    "model_c0c2.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \n",
    "model_c0c2.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "callbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\n",
    "history_c0c2 = model_c0c2.fit(x_train_c0c2, y_train_c0c2, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c2, y_test_c0c2),callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c0c2.save_weights('./driverdistraction_talking_on_the_phone_right_weights.h5', overwrite=True)\n",
    "model_c0c2.save('./driverdistraction_lr_talking_on_the_phone_right.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history_c0c2.history['acc'])\n",
    "plt.plot(history_c0c2.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history_c0c2.history['loss'])\n",
    "plt.plot(history_c0c2.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-77b8ecda42c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#creating training data for safe vs texting_left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp_array_c0c3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_c0c3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_npArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_c0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_data_c3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_size2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_size1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx_train_c0c3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test_c0c3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_c0c3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_c0c3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_array_c0c3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_c0c3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_label' is not defined"
     ]
    }
   ],
   "source": [
    "#creating training data for safe vs texting_left\n",
    "np_array_c0c3, y_c0c3 = feature_label.generate_npArray(training_data_c0,training_data_c3,img_size2,img_size1)\n",
    "x_train_c0c3,x_test_c0c3,y_train_c0c3,y_test_c0c3 = feature_label.train_and_split(np_array_c0c3,y_c0c3,0.3,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the logistic regression classifier.\n",
    "output_dim = nb_classes = 2\n",
    "batch_size = 128 \n",
    "nb_epoch = 25\n",
    "model_c0c3 = Sequential() \n",
    "model_c0c3.add(BatchNormalization())\n",
    "model_c0c3.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \n",
    "model_c0c3.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "callbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\n",
    "history_c0c3 = model_c0c3.fit(x_train_c0c3, y_train_c0c3, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c3, y_test_c0c3),callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c0c3.save_weights('./driverdistraction_Safe_texting_left_weights.h5', overwrite=True)\n",
    "model_c0c3.save('./driverdistraction_lr_Safe_texting_left.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_c0c3.history['acc'])\n",
    "plt.plot(history_c0c3.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history_c0c3.history['loss'])\n",
    "plt.plot(history_c0c3.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING TRAINING DATA FOR SAFE VS TALKING_ON_THE_PHONE_LEFT\n",
    "np_array_c0c4, y_c0c4 = feature_label.generate_npArray(training_data_c0,training_data_c4,img_size2,img_size1)\n",
    "x_train_c0c4,x_test_c0c4,y_train_c0c4,y_test_c0c4 = feature_label.train_and_split(np_array_c0c4,y_c0c4,0.3,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the logistic regression classifier.\n",
    "output_dim = nb_classes = 2\n",
    "batch_size = 128 \n",
    "nb_epoch = 25\n",
    "model_c0c4 = Sequential() \n",
    "model_c0c4.add(BatchNormalization())\n",
    "model_c0c4.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \n",
    "model_c0c4.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "callbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\n",
    "history_c0c4 = model_c0c4.fit(x_train_c0c4, y_train_c0c4, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c4, y_test_c0c4),callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c0c4.save_weights('./driverdistraction_talking_on_the_phone_left_weights.h5', overwrite=True)\n",
    "model_c0c4.save('./driverdistraction_lr_talking_on_the_phone_left.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history_c0c4.history['acc'])\n",
    "plt.plot(history_c0c4.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history_c0c4.history['loss'])\n",
    "plt.plot(history_c0c4.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating training data for Safe vs operating_the_radio\n",
    "# initializing the logistic regression classifier.\n",
    "output_dim = nb_classes = 2\n",
    "batch_size = 128 \n",
    "nb_epoch = 25\n",
    "model_c0c5 = Sequential() \n",
    "model_c0c5.add(BatchNormalization())\n",
    "model_c0c5.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \n",
    "model_c0c5.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "callbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\n",
    "history_c0c5 = model_c0c5.fit(x_train_c0c5, y_train_c0c5, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c5, y_test_c0c5),callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c0c5.save_weights('./driverdistraction_operating_the_radio_weights.h5', overwrite=True)\n",
    "model_c0c5.save('./driverdistraction_lr_operating_the_radio.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history_c0c5.history['acc'])\n",
    "plt.plot(history_c0c5.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history_c0c5.history['loss'])\n",
    "plt.plot(history_c0c5.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating training data for safe vs drinking\n",
    "np_array_c0c6, y_c0c6 = feature_label.generate_npArray(training_data_c0,training_data_c6,img_size2,img_size1)\n",
    "x_train_c0c6,x_test_c0c6,y_train_c0c6,y_test_c0c6 = feature_label.train_and_split(np_array_c0c6,y_c0c6,0.3,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the logistic regression classifier.\n",
    "output_dim = nb_classes = 2\n",
    "batch_size = 128 \n",
    "nb_epoch = 30\n",
    "model_c0c6 = Sequential() \n",
    "model_c0c6.add(BatchNormalization())\n",
    "model_c0c6.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \n",
    "model_c0c6.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "callbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\n",
    "history_c0c6 = model_c0c6.fit(x_train_c0c6, y_train_c0c6, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c6, y_test_c0c6),callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c0c6.save_weights('./driverdistraction_drinking_weights.h5', overwrite=True)\n",
    "model_c0c6.save('./driverdistraction_lr_drinking.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history_c0c6.history['acc'])\n",
    "plt.plot(history_c0c6.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history_c0c6.history['loss'])\n",
    "plt.plot(history_c0c6.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating training data for Safe vs reach_behind\n",
    "np_array_c0c7, y_c0c7 = feature_label.generate_npArray(training_data_c0,training_data_c7,img_size2,img_size1)\n",
    "x_train_c0c7,x_test_c0c7,y_train_c0c7,y_test_c0c7 = feature_label.train_and_split(np_array_c0c7,y_c0c7,0.3,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the logistic regression classifier.\n",
    "output_dim = nb_classes = 2\n",
    "batch_size = 128 \n",
    "nb_epoch = 30\n",
    "model_c0c7 = Sequential() \n",
    "model_c0c7.add(BatchNormalization())\n",
    "model_c0c7.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \n",
    "model_c0c7.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "callbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\n",
    "history_c0c7 = model_c0c7.fit(x_train_c0c7, y_train_c0c7, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c7, y_test_c0c7),callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c0c7.save_weights('./driverdistraction_reach_behind_weights.h5', overwrite=True)\n",
    "model_c0c7.save('./driverdistraction_lr_reach_behind.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-999f330bb677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot training & validation accuracy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_c0c7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_c0c7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history_c0c7.history['acc'])\n",
    "plt.plot(history_c0c7.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history_c0c7.history['loss'])\n",
    "plt.plot(history_c0c7.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating training data for safe vs hair_and_makeup\n",
    "np_array_c0c8, y_c0c8 = feature_label.generate_npArray(training_data_c0,training_data_c8,img_size2,img_size1)\n",
    "x_train_c0c8,x_test_c0c8,y_train_c0c8,y_test_c0c8 = feature_label.train_and_split(np_array_c0c8,y_c0c8,0.3,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the logistic regression classifier.\n",
    "output_dim = nb_classes = 2\n",
    "batch_size = 128 \n",
    "nb_epoch = 30\n",
    "model_c0c8 = Sequential() \n",
    "model_c0c8.add(BatchNormalization())\n",
    "model_c0c8.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \n",
    "model_c0c8.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "callbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\n",
    "history_c0c8 = model_c0c8.fit(x_train_c0c8, y_train_c0c8, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c8, y_test_c0c8),callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c0c8.save_weights('./driverdistraction_hair_and_makeup_weights.h5', overwrite=True)\n",
    "model_c0c8.save('./driverdistraction_lr_hair_and_makeup.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history_c0c8.history['acc'])\n",
    "plt.plot(history_c0c8.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history_c0c8.history['loss'])\n",
    "plt.plot(history_c0c8.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating training data for safe vs talking_to_the_passenger\n",
    "\n",
    "np_array_c0c9, y_c0c9 = feature_label.generate_npArray(training_data_c0,training_data_c9,img_size2,img_size1)\n",
    "x_train_c0c9,x_test_c0c9,y_train_c0c9,y_test_c0c9 = feature_label.train_and_split(np_array_c0c9,y_c0c9,0.3,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the logistic regression classifier.\n",
    "output_dim = nb_classes = 2\n",
    "batch_size = 128 \n",
    "nb_epoch = 25\n",
    "model_c0c9 = Sequential() \n",
    "model_c0c9.add(BatchNormalization())\n",
    "model_c0c9.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \n",
    "model_c0c9.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "callbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\n",
    "history_c0c9 = model_c0c9.fit(x_train_c0c9, y_train_c0c9, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c9, y_test_c0c9),callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c0c9.save_weights('./driverdistraction_talking_to_the_passenger_weights.h5', overwrite=True)\n",
    "model_c0c9.save('./driverdistraction_lr_talking_to_the_passenger.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history_c0c9.history['acc'])\n",
    "plt.plot(history_c0c9.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history_c0c9.history['loss'])\n",
    "plt.plot(history_c0c9.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "test_data_ = np.array(test_data[3000][1]).reshape(-1,img_size2*img_size1)\n",
    "new_img = cv2.resize(test_data[3000][1],(img_size2,img_size1))\n",
    "plt.imshow(new_img,cmap='gray')\n",
    "plt.show()\n",
    "pred = model_c0c3.predict(test_data_)\n",
    "#r2_score(y_test, pred)\n",
    "print(np.argmax(pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
